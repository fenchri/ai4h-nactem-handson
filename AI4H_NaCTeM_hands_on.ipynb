{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI4H-NaCTeM_hands-on.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "6_i1RrjWclpe"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fenchri/ai4h-nactem-handson/blob/main/AI4H_NaCTeM_hands_on.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADgPa-Rx0YnJ"
      },
      "source": [
        "# Welcome to the AI4H Winter School NaCTeM session ! ðŸŽ‰ \n",
        "\n",
        "The goal of this session is to give you a practical overview of how you can create an end-to-end pipeline model for Event Extraction. The code used in all of the following sections is packed up into a repository, which you can use to try on your own.\n",
        "After you get the grasp of the main components the of the system, you can try to change the model architecture in order to further improve performance, using methods that we discussed during the morning session.\n",
        "\n",
        "This notebook is divided into the following sections:  \n",
        "1. Set up environment\n",
        "2. Checking out the data\n",
        "3. Named Entity Recognition\n",
        "4. Relation Extraction\n",
        "5. Event Extraction\n",
        "6. Pipeline Performance\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRfWBH6JPjRE"
      },
      "source": [
        "# Import the code directory & install dependencies\n",
        "\n",
        "Replace the *\\<insert_token_here\\>* part in the next cell with the gitlab token that will be provided to you during the session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjAX2__bQI7-"
      },
      "source": [
        "!git clone https://ai4h-nactem-participants:BJ2Htni9pMiYH6cqxUcY@gitlab.com/fenchri/ai4health-nactem.git\n",
        "#!git pull"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5y84sVDZU-P"
      },
      "source": [
        "import os\n",
        "directory_path = \"/content/ai4health-nactem/\"\n",
        "os.chdir(directory_path)\n",
        "\n",
        "# Installing requirements may take a few minutes ...\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "% cd /content/ai4health-nactem/src"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ogqYqW340Ek"
      },
      "source": [
        "# Step 1: Theory\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBjL4XHttqe6"
      },
      "source": [
        "## Evaluation Metrics\n",
        "\n",
        "In order to evaluate the three networks of interest, we will incorporate the standard machine learning evaluation metrics of Preciion (P), Recall (R) and F1-score. \n",
        "In order to estimate each of these metrics we need to define the following:\n",
        "- **TP** (True Positives): Number of instances correctly identified by the model\n",
        "- **FP** (False Positives): Number of instances false identified by the model\n",
        "- **FN** (False Negatives): Number of instances missed by the model\n",
        "\n",
        "Then, we can calculate each of these metrics as follows:\n",
        "\n",
        "$ P = \\frac{TP}{TP + FP} $, $ R = \\frac{TP}{TP + FN} $, $ F1 = \\frac{2 P R }{P + R} $\n",
        "\n",
        "> Note: The sum of $TP + FP$ essentially corresponds to all *predicted* instances, while the sum of $TP + FN$ corresponds to all *true* instances.\n",
        "\n",
        "We can then estime micro- and macro-averaged version of these metrics as follows\n",
        "$ P_\\text{micro} = \\frac{\\sum_c TP_c}{\\sum_c TP_c + FP_c} $, \n",
        "$ P_\\text{macro} = \\frac{1}{|c|} \\sum_c P_c $\n",
        "\n",
        "The micro-average measures the total performance across all instances, while the macro-average measures the performance for each category separately and takes the average values across categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5i-6qFm2YsO"
      },
      "source": [
        "## Subword Segmentation\n",
        "\n",
        "While encoding words is proved to be extremelly useful, all existing methods that use word embeddings suffer from the of the out-of-vocabulary (OOV) words, due to the finite size of the vocabulary.\n",
        "A solution to this is to create **sub-word** representations to model shorter spans and thus mitigigate the OOV issue. As you can imagine, this is something very important for BioNLP since many biomedical terms are a combination of other terms, which also affect their meaning.\n",
        "\n",
        "We hightlight 3 different subword sugmentation algorithms below, some of which we will use in the remaining of the session.\n",
        "\n",
        "- **Byte Pair Encoding (BPE)**  \n",
        "Introduced by [Sennrich et al. (2016)](https://www.aclweb.org/anthology/P16-1162.pdf). It originates in data compression ([Gage, 1994](https://www.derczynski.com/papers/archive/BPE_Gage.pdf)) and the goal is to represent rare and unseen words as a sequence of subword units.\n",
        "The algorithm is an iterative process that merges adjacent characters based on their frequency of appearance. \n",
        " - Choose what to merge â†’ based on frequency\n",
        " - Merging â†’ based on frequency\n",
        "\n",
        "- **Unigram LM**\n",
        "BPE has the following drawback of creating ambiguous resulting vocabularies.\n",
        "What if there is more than one way to separate a word?\n",
        "A solution to this problem was a Probabilistic Unigram Language Model ([Kudo, 2018](https://arxiv.org/pdf/1804.10959.pdf)) that essentially \"*Chooses the most likely subword to add to the vocabulary, instead of the most frequent*\"\n",
        " - Choose what to merge â†’ based on probability\n",
        " - Merging â†’ based on probability \n",
        "\n",
        "- **WordPiece**\n",
        "This algorithm is a combination of methods we discussed before, i.e. BPE and Unigram LM. It was originally introduced by [Schuster and Nakajima (2012)](https://ieeexplore-ieee-org.manchester.idm.oclc.org/document/6289079) used in BERT LM ([Devlin et al., 2018](https://www.aclweb.org/anthology/N19-1423/))\n",
        "In the middle between BPE and Unigram LM\n",
        " - Choose what to merge â†’ based on frequency\n",
        " - Merging â†’ based on probability \n",
        "\n",
        "Specifically assuming we have a given vocabulary of specified size and set seed:\n",
        "\n",
        "1.   Find the probability of each subword (assume: subwords occur independently)\n",
        "2.   Calculate the loss of a subword in case it was removed\n",
        "3.   Keep the top X% subwords with the smallest loss (typical 80%)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCnhluvObF70"
      },
      "source": [
        "# Step 2: Checking out the data\n",
        "\n",
        "In this step we will have an initial look at the dataset we are going to use. \n",
        "In particular, we will make use of the [MLEE (Multi-level Event Extraction)](http://nactem.ac.uk/MLEE/) by [Pyysalo et al. (2012)](https://academic-oup-com.manchester.idm.oclc.org/bioinformatics/article/28/18/i575/249872) in order to perform Event Extraction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_i1RrjWclpe"
      },
      "source": [
        "## Step 2a: Inspecting the data\n",
        "\n",
        "The dataset can be parser via the [brat](https://brat.nlplab.org/) annotation tool. In the following link you can navigate across the entire collection of documents in MLEE using the arrows.\n",
        "\n",
        "http://www.nactem.ac.uk/eccb2012/index.xhtml#/10417401"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBgzuPEtb3vI"
      },
      "source": [
        "## Step 2b: Data Statistics \n",
        "\n",
        "Lets delve into the dataset statistics, in terms of named entities, events, etc.Run the following cells:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "709CTh_ZcVqJ"
      },
      "source": [
        "from statistics import calc_statistics\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_keys(report, keys, main_value, sec_value):\n",
        "  values = []\n",
        "  for key in keys:\n",
        "    if key in report[main_value][sec_value]:\n",
        "      val = report[main_value][sec_value][key]['count']\n",
        "      #print(val)\n",
        "      values.append(val)\n",
        "    else:\n",
        "      values.append(0)\n",
        "  return(values)\n",
        "\n",
        "\n",
        "report_train = calc_statistics('train')  # get statistics\n",
        "report_val = calc_statistics('val')\n",
        "report_test = calc_statistics('test')\n",
        "\n",
        "main_value = 'Entities'\n",
        "sec_value = 'Types'\n",
        "keys = list(report_test[main_value][sec_value].keys())\n",
        "\n",
        "x_train = get_keys(report_train, keys, main_value, sec_value)\n",
        "x_val = get_keys(report_val, keys, main_value, sec_value)\n",
        "x_test = get_keys(report_test, keys, main_value, sec_value)\n",
        "y = np.arange(len(keys))\n",
        "#print(y)\n",
        "ax = plt.subplot(111)\n",
        "train = ax.bar(y-0.3, x_train, width=0.3, color='tab:blue', align='center')\n",
        "val = ax.bar(y+0.0, x_val, width=0.3, color='tab:green', align='center')\n",
        "test = ax.bar(y+0.3, x_test, width=0.3, color='tab:orange', align='center')\n",
        "ax.legend( [train, val, test], ['train', 'dev', 'test'] )\n",
        "plt.rcParams['figure.figsize'] = [10, 6]\n",
        "plt.rcParams['figure.dpi'] = 90 \n",
        "plt.xticks(y, keys)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Entity Counts')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print('Total Entities:', (report_train['Entities']['Total']['count']+report_val['Entities']['Total']['count']+report_test['Entities']['Total']['count']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSJiR5D_cYMI"
      },
      "source": [
        "\n",
        "main_value = 'Triggers'\n",
        "sec_value = 'Types'\n",
        "keys = list(report_test[main_value][sec_value].keys())\n",
        "\n",
        "x_train = get_keys(report_train, keys, main_value, sec_value)\n",
        "x_val = get_keys(report_val, keys, main_value, sec_value)\n",
        "x_test = get_keys(report_test, keys, main_value, sec_value)\n",
        "\n",
        "\n",
        "y = np.arange(len(keys))\n",
        "#print(y)\n",
        "ax = plt.subplot(111)\n",
        "train = ax.bar(y-0.3, x_train, width=0.3, color='tab:blue', align='center')\n",
        "val = ax.bar(y+0.0, x_val, width=0.3, color='tab:green', align='center')\n",
        "test = ax.bar(y+0.3, x_test, width=0.3, color='tab:orange', align='center')\n",
        "ax.legend( [train, val, test], ['train', 'dev', 'test'] )\n",
        "plt.rcParams['figure.figsize'] = [10, 6]\n",
        "plt.rcParams['figure.dpi'] = 90 \n",
        "plt.xticks(y, keys)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Trigger Counts')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print('Total Triggers:', (report_train[main_value]['Total']['count']+report_val[main_value]['Total']['count']+report_test[main_value]['Total']['count']))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grZlPb2PPJ6n"
      },
      "source": [
        "\n",
        "main_value = 'Events'\n",
        "sec_value = 'Roles'\n",
        "keys = list(report_test[main_value][sec_value].keys())\n",
        "\n",
        "x_train = get_keys(report_train, keys, main_value, sec_value)\n",
        "x_val = get_keys(report_val, keys, main_value, sec_value)\n",
        "x_test = get_keys(report_test, keys, main_value, sec_value)\n",
        "\n",
        "\n",
        "y = np.arange(len(keys))\n",
        "#print(y)\n",
        "ax = plt.subplot(111)\n",
        "train = ax.bar(y-0.3, x_train, width=0.3, color='tab:blue', align='center')\n",
        "val = ax.bar(y+0.0, x_val, width=0.3, color='tab:green', align='center')\n",
        "test = ax.bar(y+0.3, x_test, width=0.3, color='tab:orange', align='center')\n",
        "ax.legend( [train, val, test], ['train', 'dev', 'test'] )\n",
        "plt.rcParams['figure.figsize'] = [10, 6]\n",
        "plt.rcParams['figure.dpi'] = 90 \n",
        "plt.xticks(y, keys)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Event Role Counts')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "total = (report_train[main_value]['Total']['count']+report_val[main_value]['Total']['count']+report_test[main_value]['Total']['count'])\n",
        "total_nested =  (report_train[main_value]['Nested']['count']+report_val[main_value]['Total']['count']+report_test[main_value]['Nested']['count'])\n",
        "print('Total Events: ', total)\n",
        "print('Nested Events: {} ({:.2f} %)'.format(total_nested, 100 * total_nested / total))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEs1aJJPWE68"
      },
      "source": [
        "print('Sentence length statistics for the train set:')\n",
        "print(str(report_train['sent_len']).replace(\"{\",\"\").replace(\"}\", \"\"))\n",
        "print('Sentence length statistics for the val set:')\n",
        "print(str(report_val['sent_len']).replace(\"{\",\"\").replace(\"}\", \"\"))\n",
        "print('Sentence length statistics for the test set:')\n",
        "print(str(report_test['sent_len']).replace(\"{\",\"\").replace(\"}\", \"\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buOh4MknbVy_"
      },
      "source": [
        "print('Entity count statistics per sentence the train set:')\n",
        "print(str(report_train['sent_ents']).replace(\"{\",\"\").replace(\"}\", \"\"))\n",
        "print('Entity count statistics per sentence for the val set:')\n",
        "print(str(report_val['sent_ents']).replace(\"{\",\"\").replace(\"}\", \"\"))\n",
        "print('Entity count statistics per sentence for the test set:')\n",
        "print(str(report_test['sent_ents']).replace(\"{\",\"\").replace(\"}\", \"\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyYQmeczo1QO"
      },
      "source": [
        "print('Event count statistics per sentence the train set:')\n",
        "print(report_train['sent_trigs'])\n",
        "print('Event count statistics per sentence for the val set:')\n",
        "print(report_val['sent_trigs'])\n",
        "print('Event count statistics per sentence for the test set:')\n",
        "print(report_test['sent_trigs'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FEI-IuFo0tc"
      },
      "source": [
        "print('Argument count statistics per event the train set:')\n",
        "print(report_train['event_args'])\n",
        "print('Argument count statistics per event for the val set:')\n",
        "print(report_val['event_args'])\n",
        "print('Argument count statistics per event for the test set:')\n",
        "print(report_test['event_args'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOZq5gYxEOUg"
      },
      "source": [
        "##Step 2c: Sample Sentence\n",
        "\n",
        "We will use the following sentence as out sample throughout the next modules.\n",
        "\n",
        "![Example  Sentence](\"/content/ai4health-nactem/images/example_sentence.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99mj68RFE6mx"
      },
      "source": [
        "from IPython.display import Image #, display\n",
        "\n",
        "\n",
        "example_image = '/content/ai4health-nactem/images/example_sentence.png'\n",
        "\n",
        "Image(example_image, width='1200')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_OYDgedXXud"
      },
      "source": [
        "# Step 3: Named Entity Recognition\n",
        "\n",
        "The first we will investigate is Named Entity Recognition (NER). \n",
        "In this task, we will identify *named entities* and *triggers* in a given sentence simultaneously. \n",
        "We will treat this problem as a **token classification** task, i.e. we predict one label for each word in the sentence.\n",
        "We will incorporate a pre-trained Language Model and finetune it on the MLEE dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdx8Og_WaPI1"
      },
      "source": [
        "## Step 3a: Creating NER instances\n",
        "\n",
        "An instance in this task is considered a sentence.\n",
        "Since we treat NER as a token classification task, we need to associate each word with a particular label. We will thus incorporate the BIO tagging scheme that we saw earlier.\n",
        "As a reminder:\n",
        "- **B-CLASS** tag is assigned to the beginning tokens\n",
        "- **I-CLASS** tag is assigned to the intermediate (and last tokens)\n",
        "- **O** tag is assigned to all other tokens (not belonginning in an entity)\n",
        "\n",
        "This is already provided by our dataset, hence each sentence is associated with a particular BIO sequence, as seen in the example below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXegp4pRbEeS"
      },
      "source": [
        "import json\n",
        "\n",
        "with open('/content/ai4health-nactem/data/MLEE_train.json', 'r') as f:\n",
        "    example = json.loads(f.readline())\n",
        "\n",
        "example_sentence = example['sentence']\n",
        "for w, l in zip(example_sentence, example['bio']):\n",
        "  print('{:<10}\\t{:<10}'.format(w, l))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIcBGQn3ZXNP"
      },
      "source": [
        "## Step 3b: Pre-processing sentences\n",
        "\n",
        "In order to feed each sentence into the model we first need to a apply a few basic pre-processing steps:\n",
        "- Tokenization\n",
        "- Subword segmentation\n",
        "- Conversion of elements to identifiers\n",
        "\n",
        "The dataset is already in a tokenized format, i.e. each word is separated by a space as we saw in step 3a. \n",
        "The most important (and tricky here) is subword tokenization.\n",
        "As we saw during the lecture, state-of-the-art language models use subword segmentation on the input sequence, before feeding it into the network.\n",
        "This means, that each word will need to be segmented based on the algorithm that each model uses (e.g. BPE, WordPiece, etc)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9Sq-QuGdBAQ"
      },
      "source": [
        "We first need to initialize a tokenizer as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHaZDN3RcMKl"
      },
      "source": [
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "            'roberta-base',\n",
        "            add_prefix_space=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c7vesE3dLV4"
      },
      "source": [
        "We can then pass the sentence into the tokenizer and inspect the output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IApKBxFdPWp"
      },
      "source": [
        "segmented_sentence = [tokenizer.tokenize(w) for w in example_sentence]\n",
        "segmented_sentence = [item for sublist in segmented_sentence for item in sublist]\n",
        "\n",
        "print('Original sentence:\\n{}\\n'.format(' | '.join(example_sentence)))\n",
        "print('Segmented sentence:\\n{}\\n'.format(' | '.join(segmented_sentence)))\n",
        "\n",
        "print(f'Original sentence length:  {len(example_sentence)}')\n",
        "print(f'Segmented sentence length: {len(segmented_sentence)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwyE-ZxWgI3m"
      },
      "source": [
        "As we can see the length of the sentence is now longer than before since for example: \n",
        "```\n",
        "extracellular --> extra ##cellular\n",
        "```\n",
        "\n",
        "This means we need to re-adjust the the labels we have, in order to have the same length in both sequences. We will do that using a \"trick\", to append an \"X\" label for each newly introduced subword.\n",
        "\n",
        "The following code shows the sentence in column 1 and the expected (extended) output labels in column 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPZ9Fb6WjdkJ"
      },
      "source": [
        "new_label_seq = []\n",
        "for w, l in zip(example_sentence, example['bio']):\n",
        "    w_ids = tokenizer.tokenize(w)\n",
        "    n_subwords = len(w_ids)  # augment labels based on number of subwords (X label)\n",
        "    new_label_seq += [l]\n",
        "    new_label_seq.extend(['X'] * (n_subwords - 1))\n",
        "\n",
        "for w, l in zip(segmented_sentence, new_label_seq):\n",
        "  print('{:<10}\\t{:<10}'.format(w, l))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p099-_QbkI3u"
      },
      "source": [
        "Now we simply need to convert each token and its label into a unique id, in order to feed them into the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNwTP1YWzwDO"
      },
      "source": [
        "from models.ner_dataset import NERdataset\n",
        "import yaml\n",
        "import torch\n",
        "from utils import *\n",
        "import yamlordereddictloader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "with open('/content/ai4health-nactem/src/config.yaml', 'r') as f:\n",
        "  config = yaml.load(f, Loader=yamlordereddictloader.Loader)\n",
        "\n",
        "config = dict(config)\n",
        "device = torch.device(\"cuda:{}\".format(config['gpu']) if config['gpu'] != -1 else \"cpu\")\n",
        "config['device'] = device\n",
        "config['task'] = 'ner'\n",
        "\n",
        "config['labels'] = ner_labels(config)\n",
        "config['labels'].trigger_types = ner_triggers(config)\n",
        "config['unique_labels'] = list(set([l.split('-')[1] \n",
        "                                    for l in config['labels'].ent2id.keys()\n",
        "                                    if '-' in l]))\n",
        "\n",
        "train_data = NERdataset(config, tokenizer, 'train')\n",
        "train_loader = DataLoader(dataset=train_data,\n",
        "                          batch_size=config['batch_size'],\n",
        "                          shuffle=True,\n",
        "                          collate_fn=train_data.collate)\n",
        "\n",
        "for trd in train_loader:\n",
        "  for i, id_ in enumerate(trd['ids']):\n",
        "    if id_ == 'PMID-16407289-s0':\n",
        "      print(' === Sentence ===')\n",
        "      print(tokenizer.convert_ids_to_tokens(trd['input_ids'][i]))\n",
        "      print(trd['input_ids'][i])\n",
        "\n",
        "      print('\\n === Labels ===')\n",
        "      print([config['labels'].id2ent[l] for l in trd['labels'][i].tolist()])\n",
        "      print(trd['labels'][i])\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pokH5jeykegG"
      },
      "source": [
        "## Step 3c: NER Module\n",
        "\n",
        "We will use a very simple and straightforward NER component, as shown in the figure below.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9YUrQh5go9E"
      },
      "source": [
        "ner_arch = '/content/ai4health-nactem/images/ner-arch.png'\n",
        "Image(ner_arch, width=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUsYrXk0NpQr"
      },
      "source": [
        "The NER module written in [PyTorch](https://pytorch.org/) is shown below:\n",
        "It essentially consists of the encoder layer (a pretrained Transformer-based LM) and a classification - linear layer or a \"softmax\" layer.\n",
        "\n",
        "The linear layer is included inside the module, stacked on top of the encoder's output.\n",
        "\n",
        "During the forward computation, the outpus of the encoder are fed into a linear layer which has as output dimensionality equal to the number of target semantic types. \n",
        "The extracted values are normalised via a softmax activation function and the predicted label is the one with the highest probability score.\n",
        "\n",
        "\n",
        "```python\n",
        "class NERmodel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(NERmodel, self).__init__()\n",
        "\n",
        "        self.config = config\n",
        "        self.num_labels = config['labels'].n_ent\n",
        "        self.model_config = AutoConfig.from_pretrained(\n",
        "            config['model_name'],\n",
        "            num_labels=self.num_labels\n",
        "        )\n",
        "        self.encoder = AutoModelForTokenClassification.from_pretrained(config['model_name'],\n",
        "                                                                       config=self.model_config)\n",
        "        self.loss_fct = nn.CrossEntropyLoss(ignore_index=config['labels'].X_id)  # ignore masked subwords\n",
        "\n",
        "    def forward(self, seqs):\n",
        "        outputs = self.encoder(seqs['input_ids'],\n",
        "                               attention_mask=seqs['attention_mask'])\n",
        "\n",
        "        logits = outputs[0]\n",
        "\n",
        "        mask = torch.ne(seqs['input_ids'], self.model_config.pad_token_id)  # remove padding\n",
        "        active_logits = logits[mask]\n",
        "        active_labels = seqs['labels'][mask]\n",
        "\n",
        "        loss = self.loss_fct(active_logits, active_labels)\n",
        "\n",
        "        return logits, loss\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0nWUA-_hUCt"
      },
      "source": [
        "### Loss Function\n",
        "\n",
        "It is important to note the loss function that we use: Cross Entropy.\n",
        "This function measures the error of the model when predicting a certain label for a certain token. It uses the true probability distribution of the labels and the predicted label distribution in order to measure how close they are. This loss forces the model to give the highest probability to the correct category.\n",
        "\n",
        "Another important aspect is the following `ignore_index=config['labels'].X_id`, which means that we want to ignore the predictions of the additional subwords - we only care about the prediction of the first token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz7Xu5N1sp9M"
      },
      "source": [
        "## Step 3d: Training\n",
        "\n",
        "In order to train our model to learn how to correctly predict labels for each token in the sentence, we will employ a trainer function.\n",
        "The trainer works in training steps, where after a full pass on the training dataset evaluates the model on the validation set.\n",
        "\n",
        "We will use the micro- and macro-averaged Precision, Recall and F1-score, as our primary metrics, in a **strict** evaluation setting, i.e. we consider an entity as **correct** if and only if both its span and semantic type are predicted correctly.\n",
        "\n",
        "\n",
        "```python\n",
        "def train_epoch(self, epoch):\n",
        "        self.model = self.model.train()\n",
        "        total_loss = 0\n",
        "        total_preds, total_truth, masks = [], [], []\n",
        "\n",
        "        iterations = len(self.loaders['train'])\n",
        "        loop = tqdm(enumerate(self.loaders['train']), total=iterations, leave=False)\n",
        "        for batch_idx, batch in loop:\n",
        "            step = ((epoch - 1) * iterations) + batch_idx\n",
        "\n",
        "            self.model.zero_grad()\n",
        "            for b in batch:\n",
        "                if b != 'ids':\n",
        "                    batch[b] = batch[b].to(self.config['device'])\n",
        "\n",
        "            logits, loss = self.model(batch)\n",
        "            preds = torch.argmax(logits, dim=2)\n",
        "\n",
        "            total_preds.extend(preds.to('cpu').data.numpy().tolist())\n",
        "            total_truth.extend(batch['labels'].to('cpu').data.numpy().tolist())\n",
        "            masks.extend(batch['attention_mask'].to('cpu').data.numpy().tolist())\n",
        "\n",
        "            # Backward & Updates\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(parameters=self.model.parameters(),\n",
        "                                           max_norm=self.config['max_grad_norm'])\n",
        "            self.optimizer.step()\n",
        "            self.scheduler.step()\n",
        "\n",
        "        scores, scores_trig, scores_ent = \\\n",
        "                performance.compute_ner_metrics(total_preds, total_truth, masks, self.config['labels'])\n",
        "        scores[\"loss\"] = total_loss / iterations\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "```python\n",
        "def eval(self, mode, per_class=False, write_preds=False):\n",
        "        self.model = self.model.eval()  # main change\n",
        "        total_loss = 0\n",
        "        total_preds, total_truth, masks, ids = [], [], [], []\n",
        "\n",
        "        iterations = len(self.loaders[mode])\n",
        "        with torch.no_grad():   # main change\n",
        "            for batch_idx, batch in enumerate(self.loaders[mode]):\n",
        "                for b in batch:\n",
        "                    if b != 'ids':\n",
        "                        batch[b] = batch[b].to(self.config['device'])\n",
        "                ...\n",
        "```\n",
        "\n",
        "In order to train the NER module, you can use the following command. This will start training the model and you can observe the performance on the training and validation sets after each epoch. \n",
        "\n",
        "It is important here to notice that performance goes up and the loss goes down after each iteration!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iZi6txdjQMJ"
      },
      "source": [
        "% cd /content/ai4health-nactem/src/\n",
        "# !python main.py --config config.yaml --mode train --task ner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEBL41d4s4YH"
      },
      "source": [
        "## Step 3e: Performance & Error Analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjixteJ3tR4v"
      },
      "source": [
        "Since training can take some time, we have already trained the model and we can simply load it for evaluation on the validation and test sets.\n",
        "\n",
        "In addition, we evaluate the performance of triggers and entities separaterly.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kql8DOd3jwF0"
      },
      "source": [
        "from evaluation import eval_ner\n",
        "import yaml\n",
        "import yamlordereddictloader\n",
        "\n",
        "with open('/content/ai4health-nactem/src/config.yaml', 'r') as f:\n",
        "  config = yaml.load(f, Loader=yamlordereddictloader.Loader)\n",
        "\n",
        "results = eval_ner('../data/MLEE_val.json', '../saved/ner-roberta-base-val_preds.json', config)\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmXwir_rkkGn"
      },
      "source": [
        "As we can observe, the model's overall performance is quite good on both the detection of named entities and triggers. \n",
        "\n",
        "However, there are several categories with very low scores. This is attributed to the small number of instances they contain in the training set - as can be confirmed from the data statistics in Step 2b.\n",
        "\n",
        "We can observe more closely the errors that the model makes using the following command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMYGnXY6tDNF"
      },
      "source": [
        "% cd /content/ai4health-nactem/src/\n",
        "!python error_analysis.py --gold ../data/MLEE_val.json --pred ../saved/ner-roberta-base-val_preds.json --task ner --config config.yaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGYIdyMplvtI"
      },
      "source": [
        "We identify three main causes of errors in the model:\n",
        "- **Type errors**: The predicted span is correct but the semantic type is wrong\n",
        "- **Span error**: The predicted span is wrong but the semantic type is correct (there should be some overlap between the gold entity and the predicted one)\n",
        "- **Wrong span, wrong type**: When both the span and the semantic type are wrong\n",
        "- **False Positive**: When the model has predicted a totally new entity which does not belong in the gold set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q48-CjmWG6zz"
      },
      "source": [
        "As it can be observed, the largest category of errors is the New entity, i.e. the model falsely predicts entities that do not exist in the annotated data. \n",
        "The second largest category is the type error, and finally the span error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sQOTxCMYSAy"
      },
      "source": [
        "# Step 4: Relation Extraction\n",
        "\n",
        "The second task  we will investigate is to predict relations between elements in a sentence, in our case, entities and triggers. We will treat this problem as a **pair classification task** and again use a pre-trained Language Model, finetuned on the MLEE dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxnIjUHgrNwO"
      },
      "source": [
        "## Step 4a: Creating relation instances\n",
        "\n",
        "In the RE task it is quite common to replicate a sentence a number of times, equal to the number of pairs contained in it. Even if this produces a lot of training instances, it is helpful for the model to focus only on a certain pair.\n",
        "\n",
        "When creating relation instances, the following should be kept in mind:\n",
        "- We need to create **negative** instances for the model to learn to predict when a pair does not share a relation\n",
        "- We need to take into account the **directionality** of the pair, so that we know which argument should come first and which should come second\n",
        "\n",
        "For our particular task, we will break-down all existing events into *relation pairs*. In particular, the *role* of an argument will serve as the relation label between a trigger-argument pair. \n",
        "Although we know that the first argument should always be a trigger, we generate entity-entity pairs as well and give them the negative relation category: \"NA\". In essence, we let the model learn that those pairs should not share any relations.\n",
        "\n",
        "Regarding *directionality*, we use the following label format:\n",
        "- 1:Type:2 --> in case the relation arrow goes from left-to-right\n",
        "- 2:Type:1 --> in case the relation should be inverse (from right-to-left)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4n99jgFGaB8"
      },
      "source": [
        "The following piece of code takes care of:\n",
        "- creation of positive and negative pairs\n",
        "- incorporation of directionality into the relation labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbWxPuy6rLkN"
      },
      "source": [
        "example_instances = []\n",
        "with open(config['train_data'], 'r') as infile:\n",
        "    for line in infile:\n",
        "        data = json.loads(line)\n",
        "\n",
        "        sentence = data['sentence']\n",
        "        print(' === Sentence === ')\n",
        "        print(' '.join(sentence)+'\\n')\n",
        "        bin_relations = {}\n",
        "        event_map = {}\n",
        "        ent_dict = {}\n",
        "\n",
        "        for ent in data['entities']:\n",
        "          ent_dict[ent['id']] = ent\n",
        "\n",
        "        print(' === Events === ')\n",
        "        for ev in data['events']:\n",
        "            print('{} ({}) -> {}'.format(\n",
        "                ent_dict[ev['trigger']]['surface'], \n",
        "                ev['event_type'], \n",
        "                [(a['role'], ent_dict[a['argument']]['surface']) for a in ev['arguments']]))\n",
        "            event_map[ev['id']] = ev['trigger']\n",
        "\n",
        "        # split into binary relations\n",
        "        for e in data['events']:\n",
        "            for arg in e['arguments']:\n",
        "                role = arg['role'].replace('1', '').replace('2', '').replace('3', '').replace('4', '')\n",
        "                if arg['argument'].startswith('E'):\n",
        "                    if arg['argument'] in event_map:\n",
        "                        bin_relations[(e['trigger'], event_map[arg['argument']])] = role\n",
        "                    else:\n",
        "                        errors += 1\n",
        "                else:\n",
        "                    bin_relations[(e['trigger'], arg['argument'])] = role\n",
        "\n",
        "        # find related things\n",
        "        print('\\n === Relation pairs ===')\n",
        "        for arg1, arg2 in combinations(data['entities'], 2):\n",
        "\n",
        "            if arg1['tokens'][-1] < arg2['tokens'][0]:  # arg1 is 1st\n",
        "                forward = (arg1['id'], arg2['id'])\n",
        "                backward = (arg2['id'], arg1['id'])\n",
        "\n",
        "                if forward in bin_relations:\n",
        "                    pair_label = '1:' + bin_relations[forward] + ':2'\n",
        "                elif backward in bin_relations:\n",
        "                    pair_label = '2:' + bin_relations[backward] + ':1'\n",
        "                else:\n",
        "                    pair_label = 'NA'\n",
        "\n",
        "                example_instances += [(sentence, arg1, arg2, pair_label)]\n",
        "                \n",
        "                if pair_label.startswith('2'):\n",
        "                  print('{} <- {} <- {}'.format(\n",
        "                      arg1['surface'], pair_label, arg2['surface']))\n",
        "                else:\n",
        "                  print('{} -> {} -> {}'.format(\n",
        "                      arg1['surface'], pair_label, arg2['surface']))\n",
        "                \n",
        "            else:  # arg1 is 2nd\n",
        "                forward = (arg2['id'], arg1['id'])\n",
        "                backward = (arg1['id'], arg2['id'])\n",
        "\n",
        "                if forward in bin_relations:\n",
        "                    pair_label = '1:' + bin_relations[forward] + ':2'\n",
        "                elif backward in bin_relations:\n",
        "                    pair_label = '2:' + bin_relations[backward] + ':1'\n",
        "                else:\n",
        "                    pair_label = 'NA'\n",
        "\n",
        "                example_instances += [(sentence, arg2, arg1, pair_label)]\n",
        "\n",
        "                if pair_label.startswith('2'):\n",
        "                  print('{} <- {} <- {}'.format(\n",
        "                      arg2['surface'], pair_label, arg1['surface']))\n",
        "                else:\n",
        "                  print('{} -> {} -> {}'.format(\n",
        "                      arg2['surface'], pair_label, arg1['surface']))\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMQb81LrvUYG"
      },
      "source": [
        "## Step 4b: Using semantic Types\n",
        "\n",
        "In order to take into accound the semantic types of the entities/triggers of a pairs we replace their surface form in text with a special semantic type token as shown below.\n",
        "\n",
        "The benefit of this technique is to avoid overfitting and memorisation of the words in the training set.\n",
        "This way, instead, we are able to learn patterns of relations!\n",
        "\n",
        "> Note: Again, we need to adjust the token offsets due to the subword segmentation!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DndnclQWSHcj"
      },
      "source": [
        "First we need to make sure, that we will add these tags as special tokens, so they will not be affected by subword segmentation! \n",
        "This is done via the `additional_special_tokens` attribute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBqhn8DkSETV"
      },
      "source": [
        "config['labels'], config['new_tokens'] = re_labels(config)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    config['model_name'],\n",
        "    add_prefix_space=True,\n",
        "    additional_special_tokens=config['new_tokens']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HU0CgG7EL5R"
      },
      "source": [
        "def insert_special_entities(sentence, arg1, arg2):\n",
        "    new_arg1 = '@' + arg1['type'] + '$'\n",
        "    new_arg2 = '@' + arg2['type'] + '$'\n",
        "\n",
        "    # insert special tokens into a sentence and update token ids\n",
        "    new_sentence = sentence[0:arg1['tokens'][0]] + \\\n",
        "                    [new_arg1] + \\\n",
        "                    sentence[(arg1['tokens'][-1] + 1):arg2['tokens'][0]] + \\\n",
        "                    [new_arg2] + \\\n",
        "                    sentence[(arg2['tokens'][-1] + 1):]\n",
        "\n",
        "    new_tokens = [len(sentence[0:arg1['tokens'][0]]),\n",
        "                  len(sentence[0:arg1['tokens'][0]]) + 1 +\n",
        "                  len(sentence[(arg1['tokens'][-1] + 1):arg2['tokens'][0]])]\n",
        "\n",
        "    # fix token ids based on BPE tokenization\n",
        "    final_tokens = [0, 0]\n",
        "    tmp_seq = []\n",
        "    for id_, w in enumerate(new_sentence):  # until 1st arg\n",
        "        w_ids = tokenizer.tokenize(w)\n",
        "        tmp_seq += w_ids\n",
        "        n_subwords = len(w_ids)  # augment id based on number of subwords\n",
        "\n",
        "        if id_ < new_tokens[0]:\n",
        "            final_tokens[0] += n_subwords\n",
        "\n",
        "        if id_ < new_tokens[1]:\n",
        "            final_tokens[1] += n_subwords\n",
        "\n",
        "    assert tmp_seq[final_tokens[0]] == new_arg1, '{} <> {}'.format(tmp_seq[final_tokens[0]], new_arg1)\n",
        "    assert tmp_seq[final_tokens[1]] == new_arg2\n",
        "\n",
        "    print('\\n'+' '.join(new_sentence))\n",
        "\n",
        "for instance in example_instances:\n",
        "  insert_special_entities(instance[0], instance[1], instance[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uf_-5DezOiGV"
      },
      "source": [
        "Again, the final step is to convert the input sequence and its corresponding labels into a set of ids before feeding it into the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIHpzAFoOoMZ"
      },
      "source": [
        "from models.re_dataset import REdataset\n",
        "\n",
        "train_data = REdataset(config, tokenizer, 'train')\n",
        "train_loader = DataLoader(dataset=train_data,\n",
        "                          batch_size=config['batch_size'],\n",
        "                          shuffle=True,\n",
        "                          collate_fn=train_data.collate)\n",
        "\n",
        "for trd in train_loader:\n",
        "  for i, id_ in enumerate(trd['ids']):\n",
        "    if id_[0] == 'PMID-16407289-s0':\n",
        "      print('\\n === Sentence ===')\n",
        "      print(tokenizer.convert_ids_to_tokens(trd['input_ids'][i]))\n",
        "      print(trd['input_ids'][i])\n",
        "\n",
        "      print('\\n === Labels ===')\n",
        "      print(config['labels'].id2rel[trd['labels'][i].item()], trd['labels'][i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71JsYuTZyLKL"
      },
      "source": [
        "## Step 4c: Relation Module\n",
        "\n",
        "Finally, we are ready to construct our relation extraction model architecture, which will be as shown in the next figure.\n",
        "\n",
        "Similarly to NER, we feed our input sequence into the model.\n",
        "> *Note*: Here our sequence has target argument replaced by their semantic types!\n",
        "\n",
        "The we take the representations from the last hidden layer of the encoder, that correspond to each one of the arguments.\n",
        "The concatenation of the two vectors is given as input to a linear classification layer, that chooses the label with the highest probability score.\n",
        "\n",
        "> *Note*: Due to our label convention we will also know the direction of the relation!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fehab8IyuB7"
      },
      "source": [
        "re_arch = '/content/ai4health-nactem/images/re-arch.png'\n",
        "Image(re_arch, width='500')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tofsYMizDiG"
      },
      "source": [
        "```python\n",
        "class REmodelPair(nn.Module):\n",
        "    \"\"\"\n",
        "    Relation Extraction Model\n",
        "    Each pair in a sentence is replaced by special tokens expressing the entity type\n",
        "    The concatenation of the special token embeddings (after the encoder) as used for classification\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(REmodelPair, self).__init__()\n",
        "\n",
        "        self.config = config\n",
        "        self.num_labels = config['labels'].n_rel\n",
        "        configuration = AutoConfig.from_pretrained(\n",
        "            config['model_name'],\n",
        "            output_hidden_states=True)\n",
        "\n",
        "        self.encoder = AutoModel.from_pretrained(\n",
        "            config['model_name'],\n",
        "            config=configuration)\n",
        "\n",
        "        self.encoder.resize_token_embeddings(\n",
        "            configuration.vocab_size + len(config['new_tokens']))\n",
        "\n",
        "        self.classifier = nn.Linear(2*configuration.hidden_size, self.num_labels)\n",
        "        self.loss_fct = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, seqs):\n",
        "        outputs = self.encoder(\n",
        "            seqs['input_ids'],\n",
        "            attention_mask=seqs['attention_mask']\n",
        "        )\n",
        "\n",
        "        rows = torch.arange(seqs['input_ids'].size(0)).long().to(self.config['device'])\n",
        "\n",
        "        first_arguments = outputs.last_hidden_state[rows, seqs['tokens'][:, 0]]\n",
        "        second_arguments = outputs.last_hidden_state[rows, seqs['tokens'][:, 1]]\n",
        "\n",
        "        pair_repr = torch.cat([first_arguments, second_arguments], dim=1)\n",
        "        logits = self.classifier(pair_repr)\n",
        "\n",
        "        loss = self.loss_fct(logits, seqs['labels'])\n",
        "\n",
        "        return logits, loss\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTno_OzpzJ4A"
      },
      "source": [
        "## Step 4d: Performance & Error Analysis\n",
        "\n",
        "Training the model will take some time, so here instead, we will evaluate an already fine-tuned model on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvHhk1mmzaVE"
      },
      "source": [
        "from evaluation import eval_re\n",
        "\n",
        "with open('/content/ai4health-nactem/src/config.yaml', 'r') as f:\n",
        "  config = yaml.load(f, Loader=yamlordereddictloader.Loader)\n",
        "\n",
        "results = eval_re('../data/MLEE_val.json', '../saved/re-roberta-base-val_preds.json', config)\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gxhk8QtUfgz"
      },
      "source": [
        "As we can see the model performance is high for the most frequent relation categories (e.g. Theme, Participant, etc). There are some categories with no gold labels, and some with very few training instances (e.g. ToLoc) where the model has poor performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMMdesHVVKyS"
      },
      "source": [
        "Moving on to the error analysis, we identify 4 types of potential errors in the model:\n",
        "- **Type Error**: Where the detected direction is correct, but the relation category is wrong\n",
        "- **Direction Error**: Where the detected category is correct, but the direction is wrong\n",
        "- **Entity-Entity Connection**: Where the model detects two named entities as sharing a relation\n",
        "> *Note*: Here we wanted to let the model learn that these pairs shouldn't be related in our task\n",
        "- **False positive**: Where the model detects a relation between a new pair (not sharing a relation in the gold data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NMluHfuzOiR"
      },
      "source": [
        "!python error_analysis.py --config config.yaml --task re --gold ../data/MLEE_val.json --pred ../saved/re-roberta-base-val_preds.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHjpEiPPV4tR"
      },
      "source": [
        "As we can observe, the most common cause of erros are the false positives.\n",
        "The second most common error is the type error while the model learns pretty well to distinguish the direction of the relation, as well as that entity-entity pairs shouldn't be related."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gday_rSqYVnu"
      },
      "source": [
        "# Step 5: Event Extraction\n",
        "\n",
        "The final task (for this session) is to learn how to combine binary relations, such as the relation pairs identified in the previous section into **event structures**.\n",
        "\n",
        "We treat this as a binary classification task: For each generated event structure the model predicts whether it is valid (label = 1) or invalid (label = 0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51CofOMe_uPX"
      },
      "source": [
        "## Step 5a: Event Structure Representation\n",
        "\n",
        "For the scope of this session we consider we consider events as a set of relationship (argument) tuples linking NEs to the same trigger entity: \n",
        "\n",
        "> Event = {ARG_1, ARG_2, ..., ARG_n}\n",
        ">> ARG_n = (\\<trigger\\>, \\<argument_n\\>, \\<ROLE\\>)\n",
        "\n",
        "Revisiting our sample sentence:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VzoOlDmO_pM"
      },
      "source": [
        "% cd /content/ai4health-nactem/src\n",
        "import json\n",
        "from IPython.display import Image #, display\n",
        "with open('../data/MLEE_train.json', 'r') as f:\n",
        "    example = json.loads(f.readline())\n",
        "\n",
        "example_sentence = example['sentence']\n",
        "entity_dict={}\n",
        "for entity in example['entities']:\n",
        "  eid = entity['id']\n",
        "  entity_dict[eid] = entity\n",
        "example_event = example['events'][0]\n",
        "for arg in example_event['arguments']:\n",
        "  arg['type']=entity_dict[arg['argument']]['type']\n",
        "  arg['id']=arg['argument']\n",
        "\n",
        "trigger = example_event['trigger']+\"|\"+example_event['event_type']\n",
        "args = []\n",
        "for arg in example_event['arguments']:\n",
        "  arg_rep = arg['id']+\"|\"+arg['type']\n",
        "  args.append(tuple([trigger,arg_rep, arg['role']]))\n",
        "example_image = '/content/ai4health-nactem/images/example_sentence.png'\n",
        "Image(example_image, width='1200')\n",
        "\n",
        "print()\n",
        "print('\\033[1m' + str(example_event['id']+'-->'+str(args)) + '\\033[0m')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TorKbeDsQL2V"
      },
      "source": [
        "### Nested events:\n",
        "Flat events are easy to directly represent as described above.\n",
        "\n",
        "The above representation does not consider the sub-structures in the case where the argument is another event trigger. \n",
        "So for a nested event as follows we would have:\n",
        "\n",
        "\n",
        "Full event representation:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnX-I-8kvpd8"
      },
      "source": [
        "from IPython.display import Image #, display\n",
        "\n",
        "example_image_n = '/content/ai4health-nactem/images/example_nested.png'\n",
        "Image(example_image_n, width='1200')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH89SQdqvqOU"
      },
      "source": [
        "Our simplified event representation however would be:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5lzru5pxbW-"
      },
      "source": [
        "example_image_nf = '/content/ai4health-nactem/images/example_nested_flat.png'\n",
        "Image(example_image_nf, width='1200')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOG8QRT-xduI"
      },
      "source": [
        "As such, the following structures would be equivalent:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cGoAvahxcCA"
      },
      "source": [
        "example_image_np = '/content/ai4health-nactem/images/example_nested_perm.png'\n",
        "Image(example_image_np, width='1200')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvDB4ADKAY-m"
      },
      "source": [
        "## Step 5b: Event Structure Generation\n",
        "\n",
        "How do we generate training instances? Some concerns\n",
        "\n",
        "\n",
        "*   In different sentences the same event type may have a **varying number of arguments**\n",
        "*   Some valid events have **no arguments**\n",
        "*   The same event type may have **different role-entity** argument pairs\n",
        "\n",
        "**Solution 1:** Exhaustive \\\\\n",
        "(Argument/role agnostic) \\\\\n",
        "Generate exhaustively all possible (Trigger, Entity, Role) triplets for each sentence\n",
        "\n",
        "Training set: 500K instances / 0.1% positive\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhhkwULL3_On"
      },
      "source": [
        "**Hint:** Some events would never take a specific role | entity type as argument\n",
        "\n",
        "**Solution 2:** Valid template generator \\\\\n",
        "(Argument aware) \\\\\n",
        "For each event type, generate all possible argument combinations that have appeared in the dataset at least once.\n",
        "\n",
        "Training set: 194K instances / 2%\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kszyk9aM3KLk"
      },
      "source": [
        "from itertools import permutations, combinations, product, chain\n",
        "from operator import itemgetter\n",
        "templates = {}\n",
        "unique_templates = {}\n",
        "total_templates = 0\n",
        "total_unique_templates = 0\n",
        "non_existent_arg = 0\n",
        "\n",
        "with open('../data/MLEE_train.json', 'r') as infile:\n",
        "  for line in infile:\n",
        "    data = json.loads(line)\n",
        "    e_type = {e['id']: e['type'] for e in data['entities']}\n",
        "    event_trig_map = {ev['id']: ev['trigger'] for ev in data['events']}\n",
        "\n",
        "    for e in data['events']:\n",
        "      structure = []\n",
        "\n",
        "      if e['event_type'] not in templates:\n",
        "        templates[e['event_type']] = []\n",
        "        unique_templates[e['event_type']] = []\n",
        "\n",
        "      for arg in e['arguments']:\n",
        "        role = arg['role'].replace('1', '').replace('2', '').replace('3', '').replace('4', '')\n",
        "\n",
        "        if arg['argument'].startswith('T'):\n",
        "            structure.append((role, e_type[arg['argument']]))\n",
        "        else:\n",
        "            if arg['argument'] in event_trig_map:\n",
        "                structure.append((role, e_type[event_trig_map[arg['argument']]]))\n",
        "            else:\n",
        "                structure = []  # exclude event\n",
        "                non_existent_arg += 1\n",
        "\n",
        "      if structure:\n",
        "          perm_structure = list(permutations(structure, len(structure)))\n",
        "          templates[e['event_type']] += perm_structure\n",
        "          unique_templates[e['event_type']].append(tuple(structure))\n",
        "\n",
        "\n",
        "    # remove duplicates\n",
        "for type_ in templates:\n",
        "  templates[type_] = list(set(templates[type_]))\n",
        "  # unique_templates[type_] = list(set(unique_templates[type_]))\n",
        "\n",
        "  total_templates += len(templates[type_])\n",
        "\n",
        "  unique_templates[type_].sort(key=itemgetter(0))\n",
        "  unique_templates[type_] = list(set(unique_templates[type_]))\n",
        "  total_unique_templates += len(unique_templates[type_])\n",
        "\n",
        "print('Total Templates: {}'.format(total_templates))\n",
        "print('Total Unique Templates: {}'.format(total_unique_templates))\n",
        "print('Event_templates for: Binding:' )\n",
        "print(\"\\n\".join(\"'{}'\".format(value) for value in unique_templates['Binding']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEBz4sxdHVR4"
      },
      "source": [
        "Sub-case: sample randomly from the generated negative instances:\n",
        "\n",
        "[code snippet from models/ee_dataset.py]\n",
        "\n",
        "```python\n",
        "if len(all_negative_events)>10 and self.mode=='train':\n",
        "  sub_indices = random.sample(range(0, len(all_negative_events)), 10)\n",
        "  sub_neg_events = [all_negative_events[index] for index in sub_indices]\n",
        "  sub_neg_labels = [0]*len(sub_neg_events)\n",
        "  return sub_neg_events, sub_neg_labels\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Training set: 25K instances / 16% positive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLsoDWzo5IvD"
      },
      "source": [
        "> Could we further constrain the generation of negative instances for training?\n",
        "\n",
        "**Hint:** Assuming a perfect RE model, the EE model would only have to choose among partial events\n",
        "\n",
        "**Solution 3:** Partial event generator \\\\\n",
        "(Event aware) \\\\\n",
        "For each event, generate all possible partial sub events (including a no-argument instance). \n",
        "\n",
        "Training set: 11K instances / 30% positive\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqfz0CglAnp0"
      },
      "source": [
        "## Step 5c: Event Structure Encoding\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6IG7J1v67OR"
      },
      "source": [
        "In order to encode all te event-related information in the sentence we would need to be able to:\n",
        "\n",
        "*   Combine entity and role information in event encoding\n",
        " *   Identify entities (including triggers) in sentence and replace with dedicated \\<type\\> tokens that will not be further splitted by the tokenizer. This way we also ensure that rare or unseen entities will still be encoded based on their type and not their .\n",
        " *   Maintain sentence splitting\n",
        "\n",
        "So we would like to have a re-tokenization as follows:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RFqTl3UBqMk"
      },
      "source": [
        "\n",
        "sentence_after = ['[Regulation]', 'of', 'the', 'composition', 'of', 'the', '[Cellular_component]', 'by', '[Gene_or_gene_product]', ':', 'activities', 'based', 'on', 'regulation', 'of', 'mRNA', 'expression', '.']\n",
        "sentence_before = ['Regulation', 'of', 'the', 'composition', 'of', 'the', 'extracellular', 'matrix', 'by', 'low', 'density', 'lipoprotein', 'receptor', '-', 'related', 'protein', '-', '1', ':', 'activities', 'based', 'on', 'regulation', 'of', 'mRNA', 'expression', '.']\n",
        "print('Word splitted sentence:')\n",
        "print(sentence_before)\n",
        "print('Triggers | Entities replaced:')\n",
        "print(sentence_after)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmKgy6Kl0C9x"
      },
      "source": [
        "To achieve this we parse all triggers, entities and role in all data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkvxuWwA66nY"
      },
      "source": [
        "%cd src\n",
        "config_data = {'train_data': '../data/MLEE_train.json',\n",
        "    'val_data': '../data/MLEE_val.json',\n",
        "    'test_data': '../data/MLEE_test.json'\n",
        "}\n",
        "\n",
        "\n",
        "def event_specific_tokens():\n",
        "    trigger_types = find_trigger_types()\n",
        "    #print(trigger_types)\n",
        "    entity_types = find_entity_types( trigger_types)\n",
        "    #print(entity_types)\n",
        "    #role_types, role_types_tokens = find_role_types()\n",
        "    #role_types_tokens.sort()\n",
        "    #print(role_types_tokens)\n",
        "\n",
        "    token_dict = {\n",
        "        \"triggers\":trigger_types,\n",
        "        \"entities\":entity_types\n",
        "        #\"roles\":role_types\n",
        "    }\n",
        "    all_new_tokens = token_dict['triggers'] + token_dict['entities'] \n",
        "    return token_dict, all_new_tokens\n",
        "\n",
        "\n",
        "def find_entity_types(trigger_types):\n",
        "    entity_types = []\n",
        "\n",
        "    for mode in ['train', 'val', 'test']:\n",
        "        with open(config_data[mode + '_data'], 'r') as infile:\n",
        "            for line in infile:\n",
        "                data = json.loads(line)\n",
        "\n",
        "                for e in data['entities']:\n",
        "                    ent = e['type']\n",
        "                    if not ent in trigger_types:\n",
        "                        entity_types += ['['+ent+']']\n",
        "\n",
        "    entity_types = list(set(entity_types))\n",
        "    entity_types.sort()\n",
        "    return entity_types\n",
        "\n",
        "def find_trigger_types():\n",
        "    trigger_types = []\n",
        "\n",
        "    for mode in ['train', 'val', 'test']:\n",
        "        with open(config_data[mode + '_data'], 'r') as infile:\n",
        "            for line in infile:\n",
        "                data = json.loads(line)\n",
        "\n",
        "                for e in data['events']:\n",
        "                    evt = e['event_type']\n",
        "                    trigger_types += ['['+evt+']']\n",
        "\n",
        "    trigger_types = list(set(trigger_types))\n",
        "    trigger_types.sort()\n",
        "    return trigger_types\n",
        "\n",
        "new_event_tokens, new_tokens = event_specific_tokens()\n",
        "\n",
        "print(\"We will add the following new tokens:\")\n",
        "print(\"\\n\".join(\"'{}'\".format(value) for value in new_tokens))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pgn6SesWVyeF"
      },
      "source": [
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "            'roberta-base',\n",
        "            add_prefix_space=True,\n",
        "            additional_special_tokens=new_tokens\n",
        "        )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ga8R46Y2ps3"
      },
      "source": [
        "> We also want to be able to encode some information about the role of each argument:\n",
        "\n",
        "However, the role is not represented by a token within the sentence, so we will need to train specific **role embeddings** to encode the role. This can be a small vecrtor, which will work as a look=up table for roles. Similar to our vocabulary, but instead of learning word representation in a sentence it will learn role representations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNqcQ9pako64"
      },
      "source": [
        "def find_role_types():\n",
        "    role_types = []\n",
        "    role_types_tokens = []\n",
        "    role_types_tokens += ['[NOARG]']\n",
        "    for mode in ['train', 'val', 'test']:\n",
        "        with open(config_data[mode + '_data'], 'r') as infile:\n",
        "            for line in infile:\n",
        "                data = json.loads(line)\n",
        "                for e in data['events']:\n",
        "                    arguments = e['arguments']\n",
        "                    for arg in arguments:\n",
        "                        role = arg['role'].replace('1', '').replace('2', '').replace('3', '').replace('4', '')\n",
        "                        role_types_tokens+=['['+role+']']\n",
        "                        role_types+=[role]\n",
        "\n",
        "    role_types = list(set(role_types))\n",
        "    role_types.sort()\n",
        "    role_types_tokens = list(set(role_types_tokens))\n",
        "    \n",
        "    #print(role_types)\n",
        "    #print(role_types_tokens)\n",
        "    return role_types, role_types_tokens\n",
        "\n",
        "role_tokens, role_emb_tokens = find_role_types()\n",
        "print(role_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwMOr0Yi4CML"
      },
      "source": [
        "> At this point we are ready to process the sentence:\n",
        "\n",
        "* Identify and store the position of tokens of interest\n",
        "* Identify the ids of roles for each entity corresponding to an argument and \n",
        " * Store them in order so that we can retrieve them and map them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qpc-S2-SLB3p"
      },
      "source": [
        "def insert_special_tokens(tokenized_sentence, event, roles_tokens):\n",
        "        #TODO add parameter for CLS      \n",
        "        element_dict = {} #start_token:element\n",
        "        trigger_token = -1\n",
        "        arg_tokens = []\n",
        "        role_token_ordered=[]\n",
        "  \n",
        "        trigger_element = [event['event_type'],event['trigger_tokens'],'TRIGGER']\n",
        "        #print(event['trigger_tokens'])\n",
        "        trigger_start = event['trigger_tokens'][0]\n",
        "        element_dict[trigger_start] = trigger_element\n",
        "        for arg in event['arguments']:\n",
        "            arg_element = [arg['type'], arg['tokens'], arg['role']]\n",
        "            arg_start = arg['tokens'][0]\n",
        "            element_dict[arg_start] = arg_element\n",
        "\n",
        "        #order so that we adapt the token indexes accordingly\n",
        "        ordered_element_dict = collections.OrderedDict(sorted(element_dict.items()))\n",
        "        shift_cnt = 0\n",
        "        for key, element in ordered_element_dict.items():\n",
        "            special_token = '['+element[0]+']'\n",
        "            tokens = element[1]\n",
        "            tokens = [x-shift_cnt for x in tokens]\n",
        "            RT = element[2]\n",
        "        \n",
        "            tokenized_sentence = tokenized_sentence[0:tokens[0]] + \\\n",
        "                       [special_token] + \\\n",
        "                       tokenized_sentence[(tokens[-1]+1):]\n",
        "            shift_cnt+=len(tokens)-1\n",
        "            extra_length = 1 #CLS \n",
        "            for token in tokenized_sentence[0:tokens[0]]:\n",
        "             \n",
        "                encoded_token_length = len(tokenizer.tokenize(token))\n",
        "                extra_length+=encoded_token_length-1\n",
        "            stoken=tokens[0]+extra_length\n",
        "\n",
        "            if RT == 'TRIGGER':\n",
        "                trigger_token = stoken \n",
        "            else:\n",
        "                arg_tokens += [stoken]\n",
        "                role_token_ordered+=[RT]\n",
        "        \n",
        "            full_sent = tokenizer.tokenize(\n",
        "                              tokenized_sentence, \n",
        "                              is_split_into_words=True, \n",
        "                              add_special_tokens=True)\n",
        "            assert full_sent[stoken]==special_token\n",
        "                  \n",
        "        assert len(role_token_ordered)==len(arg_tokens)\n",
        "        new_tokens = [trigger_token]+arg_tokens\n",
        "        new_tokens= [0]+new_tokens\n",
        "        role_ids = []\n",
        "        for role_tok in role_token_ordered:\n",
        "            role_tok = role_tok.replace('1', '').replace('2', '').replace('3', '').replace('4', '')\n",
        "            role_ids.append(roles_tokens.index(role_tok))\n",
        "       # print('Adding tokens for:')\n",
        "       # print(event)\n",
        "        arglen = len(event['arguments'])\n",
        "       # print(arglen)\n",
        "       # print(new_tokens)\n",
        "        assert arglen == len(new_tokens)-2\n",
        "        return tokenized_sentence, new_tokens, role_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MqZ12l6hPLp"
      },
      "source": [
        "import json\n",
        "import collections\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "            'roberta-base',\n",
        "            add_prefix_space=True,\n",
        "            additional_special_tokens=new_tokens\n",
        "        )\n",
        "with open('/content/ai4health-nactem/data/MLEE_train.json', 'r') as f:\n",
        "    example = json.loads(f.readline())\n",
        "\n",
        "example_sentence = example['sentence']\n",
        "entity_dict={}\n",
        "for entity in example['entities']:\n",
        "  eid = entity['id']\n",
        "  entity_dict[eid] = entity\n",
        "example_event = example['events'][0]\n",
        "tid = example_event['trigger']\n",
        "example_event['trigger_tokens'] = entity_dict[tid]['tokens']\n",
        "for arg in example_event['arguments']:\n",
        "  arg['type']=entity_dict[arg['argument']]['type']\n",
        "  arg['id']=arg['argument']\n",
        "  arg['tokens']=entity_dict[arg['id']]['tokens']\n",
        "tokenized_sentence, new_token_ids, role_ids = insert_special_tokens(example_sentence, example_event, role_tokens)\n",
        "\n",
        "print(\" \".join(example_sentence))\n",
        "print(\" \".join(tokenized_sentence))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XS4vs7x5mBS5"
      },
      "source": [
        "> We also retain the *expected* new tokens for the special tokens, i.e., we anticipate further subword splitting. \n",
        "\n",
        "We also facilitate potential use of the special start (sentence) embedding, by saving at the start of the token indices. As such, the actual token indices start from teh second position in the vector, with the `trigger_token_id` being first, and the arguments following in order.\n",
        "\n",
        "So initially the indices may seem wrong!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXcVDcHGmdMK"
      },
      "source": [
        "print(\" \".join(tokenized_sentence))\n",
        "\n",
        "print(new_token_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2snoaOOJA0I"
      },
      "source": [
        "###Retokenizing the sentence:\n",
        "\n",
        "We now pas the sentence with the special tokens through the tokenizer of choice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8alT1ltKWkn6"
      },
      "source": [
        "re_tokenized_sentence = tokenizer.tokenize(\n",
        "                              tokenized_sentence, \n",
        "                              is_split_into_words=True, \n",
        "                              add_special_tokens=True)\n",
        "print(\" \".join(re_tokenized_sentence))\n",
        "print(new_token_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wrziIa7CimO"
      },
      "source": [
        "### EE module architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrhwfoScIJ3R"
      },
      "source": [
        "ee_arch = '/content/ai4health-nactem/images/ee-module.png'\n",
        "Image(ee_arch, width='800')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBaUs55uAnfE"
      },
      "source": [
        "## Step 5d: EE Module\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "class EEmodel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(EEmodel, self).__init__()\n",
        "        self.config = config\n",
        "        self.num_args = config['num_args']\n",
        "        self.num_labels = 2\n",
        "        configuration = AutoConfig.from_pretrained(config['model_name'],\n",
        "                                                   num_labels=self.num_labels)\n",
        "\n",
        "        self.role_embed = nn.Embedding(num_embeddings=len(self.config['roles']),\n",
        "                                       embedding_dim=self.config['embedding_dim'])\n",
        "\n",
        "        self.encoder = AutoModel.from_pretrained(config['model_name'],\n",
        "                                                 config=configuration)\n",
        "        self.encoder.resize_token_embeddings(\n",
        "            configuration.vocab_size + len(config['new_tokens']))\n",
        "\n",
        "        self.event_classifier = nn.Linear(configuration.hidden_size, 2)\n",
        "        self.arg_layer = nn.Linear(2*configuration.hidden_size+config['embedding_dim'], configuration.hidden_size)\n",
        "        self.loss_fct = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "    def forward(self, seqs):\n",
        "        outputs = self.encoder(seqs['input_ids'],\n",
        "            attention_mask=seqs['attention_mask'])\n",
        "       \n",
        "        labels = seqs['labels']\n",
        "        arglen = seqs['arglen'][0]\n",
        "        rows = torch.arange(seqs['input_ids'].size(0)).long().to(self.config['device'])\n",
        "        trigger = outputs.last_hidden_state[rows, seqs['tokens'][:, 1]]\n",
        "        event_rep = trigger\n",
        "     \n",
        "        arg_repi = [] \n",
        "        for i in range(0,arglen):\n",
        "            argument = outputs.last_hidden_state[rows, seqs['tokens'][:, i+2]]\n",
        "            role_embedding = self.role_embed(seqs['roles'][:,i])   \n",
        "            arg_l = torch.cat([trigger, argument, role_embedding], dim=1)\n",
        "            arg_rep = self.arg_layer(arg_l).unsqueeze(1)\n",
        "            arg_repi.append(arg_rep)\n",
        "            arg_all_rep = torch.cat(arg_repi, dim=1)\n",
        "            event_rep = torch.mean(arg_all_rep, dim=1)\n",
        "        \n",
        "        logits = self.event_classifier(event_rep)\n",
        "        loss = self.loss_fct(logits, labels)\n",
        "        \n",
        "        return logits, loss\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeOHzLHok8qE"
      },
      "source": [
        "### Dataloading and passing to the model:\n",
        "\n",
        "While standard Sampler and Dataloader modules are provided by pytorch, some customisation will help"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB1IHt1yWxms"
      },
      "source": [
        "Event instances are passed to the model in **batches**. Hence all the operations in the forward function are performed on tensor vectors, and dimensionality must be the same.\n",
        "> For the event representation layer, we assume uniform argument length per batch\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "arg_repi = [] \n",
        "for i in range(0,arglen):\n",
        "  argument = outputs.last_hidden_state[rows, seqs['tokens'][:, i+2]]\n",
        "  role_embedding = self.role_embed(seqs['roles'][:,i])   \n",
        "  arg_l = torch.cat([trigger, argument, role_embedding], dim=1)\n",
        "  arg_rep = self.arg_layer(arg_l).unsqueeze(1)\n",
        "  arg_repi.append(arg_rep)\n",
        "  arg_all_rep = torch.cat(arg_repi, dim=1)\n",
        "  event_rep = torch.mean(arg_all_rep, dim=1)\n",
        "  ```\n",
        "\n",
        "\n",
        "... but events do not have the same number of arguments...\n",
        "\n",
        "For this reason we need to customise the batch loader and make it so that events are sampled per argument length. Do do so we also need to pass the argument_length information to the batch data loader. So in total, the vector passed for each event instance, will contain:\n",
        "\n",
        "* The tokenized sentence ids \n",
        "* The indexes for the trigger token, and the argument tokens\n",
        "* The argument length\n",
        "* The role embedding ids\n",
        "* The label \n",
        "\n",
        " <font color='gray'>+ Some unique id for the instance so that we can retrieve other information</font>\n",
        "\n",
        "\n",
        "\n",
        ">Then the batch loader can be adated: Check the BucketBatchSampler in models/ee_dataset.py\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcJYtrF0Q84z"
      },
      "source": [
        "\n",
        "\n",
        "```python\n",
        "class BucketBatchSampler(Sampler):\n",
        "    \"\"\"\n",
        "    Defines a strategy for drawing batches of samples from the dataset,\n",
        "    in ascending or descending order, based in the event argument number.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, arg_len, batch_size, strict = True,\n",
        "                 shuffle=False, even=False, drop_last=False, reverse=False):\n",
        "        sorted_indices = numpy.array(arg_len).argsort()\n",
        "        num_sections = math.ceil(len(arg_len) / batch_size)\n",
        "        split_lengths = []\n",
        "        if even:\n",
        "            self.batches = list(self.divide_chunks(sorted_indices, batch_size))\n",
        "        else:\n",
        "            self.batches = numpy.array_split(sorted_indices, num_sections)\n",
        "            \n",
        "            arg_len.sort()\n",
        "            \n",
        "            split_lengths = numpy.array_split(arg_len, num_sections)\n",
        "\n",
        "        if reverse:\n",
        "            self.batches = list(reversed(self.batches))\n",
        "\n",
        "        if drop_last:\n",
        "            del self.batches[-1]\n",
        "\n",
        "        if strict:\n",
        "            tbr = []\n",
        "            tba = []\n",
        "            for i,l in enumerate(split_lengths):\n",
        "                batch = self.batches[i]\n",
        "                if l[0]!=l[-1]:\n",
        "                    split_idx = numpy.count_nonzero(l == l[0])\n",
        "                    b1 = batch[0:split_idx]\n",
        "                    b2 = batch[split_idx:]\n",
        "                    tbr.append(i)\n",
        "                    tba += [b1,b2]\n",
        "                    \n",
        "            for i,b in enumerate(tbr):\n",
        "                \n",
        "                del self.batches[b-i]\n",
        "            for b in tba:\n",
        "                self.batches.append(b)\n",
        "            \n",
        "        self.shuffle = shuffle\n",
        "\n",
        "    def __iter__(self):\n",
        "        if self.shuffle:\n",
        "            return iter(self.batches[i] for i in torch.randperm(len(self.batches)))\n",
        "        else:\n",
        "            return iter(self.batches)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.batches)\n",
        "\n",
        "    @staticmethod\n",
        "    def divide_chunks(l, n):\n",
        "        # looping till length l\n",
        "        for i in range(0, len(l), n):\n",
        "            yield l[i:i + n]\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAzDl1_x1BNY"
      },
      "source": [
        "example_image = '/content/ai4health-nactem/images/batch.png'\n",
        "\n",
        "Image(example_image, width='300')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKys-C-61N6Z"
      },
      "source": [
        "Then we just pass our custom batch sampler as an argument:\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "train_sampler = BucketBatchSampler(train_data.get_lengths(),\n",
        "                                       config[\"batch_size\"],\n",
        "                                       shuffle=True)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_data,\n",
        "                      #batch_size=config['batch_size'],\n",
        "                      batch_sampler=train_sampler,\n",
        "                      #shuffle=True,\n",
        "                      collate_fn=train_data.collate)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfKhyKRvR0eu"
      },
      "source": [
        "### Training, Testing\n",
        "\n",
        "Run this after the session, it will take a while..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X87QQNOkYNTr"
      },
      "source": [
        "#% python main.py --config config.yaml --mode train --task ee\n",
        "#% python main.py --config config.yaml --mode test --task ee"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOJvv4WCBAB4"
      },
      "source": [
        "## Step 5e: Performance & Error Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euNOICQRxYjq"
      },
      "source": [
        "def produce_report_ee(stats_dict):\n",
        "    per_class = {}\n",
        "    report = {}\n",
        "    tp, tn, fp, fn, actual, pred = 0, 0, 0, 0, 0, 0\n",
        "    total_p, total_r = [], []\n",
        "    for label in stats_dict:\n",
        "        per_class[label] = calc_prf1(stats_dict[label])\n",
        "\n",
        "        tp += stats_dict[label]['tp']\n",
        "        actual += stats_dict[label]['actual']\n",
        "        pred += stats_dict[label]['pred']\n",
        "\n",
        "        total_p += [per_class[label]['p']]\n",
        "        total_r += [per_class[label]['r']]\n",
        "\n",
        "    macro_p = np.round(np.mean(total_p), 4)\n",
        "    macro_r = np.round(np.mean(total_r), 4)\n",
        "\n",
        "    report['micro'] = calc_prf1({'tp': tp, 'pred': pred, 'actual': actual})\n",
        "    report['macro'] = {'p': macro_p, 'r': macro_r, 'f1': np.round(calc_f1(macro_p, macro_r), 4),\n",
        "                       'TP': 0, 'ACTUAL': 0, 'PRED': 0}\n",
        "\n",
        "    report['per_class'] = per_class\n",
        "    report['per_class']['_MICRO_'] = report['micro']\n",
        "    report['per_class']['_MACRO_'] = report['macro']\n",
        "    return report\n",
        "\n",
        "def calc_prf1(sdict):\n",
        "    precision = calc_prec(sdict['tp'], sdict['pred'])\n",
        "    recall = calc_rec(sdict['tp'], sdict['actual'])\n",
        "    f1 = calc_f1(precision, recall)\n",
        "\n",
        "    return {'p': round(precision, 4),\n",
        "            'r': round(recall, 4),\n",
        "            'f1': round(f1, 4),\n",
        "            'TP': sdict['tp'], 'ACTUAL': sdict['actual'], 'PRED': sdict['pred']}\n",
        "\n",
        "def calc_f1(p, r):\n",
        "    return (2 * p * r) / (p + r) if (p != 0 and r != 0) else 0\n",
        "\n",
        "\n",
        "def calc_prec(tp, pred):\n",
        "    return tp / pred if (tp != 0 and pred != 0) else 0\n",
        "\n",
        "\n",
        "def calc_rec(tp, actual):\n",
        "    return tp / actual if (tp != 0 and actual != 0) else 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03FRVVVWxihI"
      },
      "source": [
        "import performance\n",
        "from performance import produce_report_ee"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2iXVHCZUZ5s"
      },
      "source": [
        "% cd /content/ai4health-nactem/src\n",
        "input_val  = \"/content/ai4health-nactem/saved/ee-roberta-base-val_eval.csv\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "predictions = pd.read_csv(input_val)\n",
        "predictions.columns = ['predicted_value', 'true_value', 'event_type','num_args', 'label']\n",
        "event_types = predictions['event_type']\n",
        "unique_event_types = list(set(event_types))\n",
        "unique_event_types.sort()\n",
        "\n",
        "stats_dict_types = {r: {'pred': 0, 'actual': 0, 'tp': 0, 'tn':0, 'fp':0, 'fn':0} for r in unique_event_types}\n",
        "stats_dict_lens = {r: {'pred': 0, 'actual': 0, 'tp': 0, 'tn':0, 'fp':0, 'fn':0} for r in [0,1,2,3,4]}\n",
        "for index, row in predictions.iterrows():\n",
        "    y = row['true_value']\n",
        "    p = row['predicted_value']\n",
        "    typeev =row['event_type']\n",
        "    arglen = row['num_args']\n",
        "    if y==p:\n",
        "        if y==1:\n",
        "            stats_dict_types[typeev]['tp']+=1\n",
        "            stats_dict_types[typeev]['pred']+=1\n",
        "            stats_dict_types[typeev]['actual']+=1\n",
        "            \n",
        "            stats_dict_lens[arglen]['tp']+=1\n",
        "            stats_dict_lens[arglen]['pred']+=1\n",
        "            stats_dict_lens[arglen]['actual']+=1\n",
        "        if y==0:\n",
        "            stats_dict_types[typeev]['tn']+=1\n",
        "            \n",
        "            stats_dict_lens[arglen]['tn']+=1\n",
        "    else:\n",
        "        if y==1:\n",
        "            stats_dict_types[typeev]['fn']+=1\n",
        "            stats_dict_types[typeev]['actual']+=1\n",
        "            \n",
        "            stats_dict_lens[arglen]['fn']+=1\n",
        "            stats_dict_lens[arglen]['actual']+=1\n",
        "        if y==0:\n",
        "            stats_dict_types[typeev]['fp']+=1\n",
        "            stats_dict_types[typeev]['pred']+=1\n",
        "            \n",
        "            stats_dict_lens[arglen]['fp']+=1\n",
        "            stats_dict_lens[arglen]['pred']+=1\n",
        "\n",
        "report_length_val = produce_report_ee(stats_dict_lens)\n",
        "report_types_val = produce_report_ee(stats_dict_types)\n",
        "\n",
        "print(\"Performance on validation set\")\n",
        "dataf = pd.DataFrame.from_dict(report_types_val['per_class'], orient='index')\n",
        "print(dataf.round(4))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUas18SbeK2F"
      },
      "source": [
        "input_test  = \"../saved/ee-roberta-base-test_eval.csv\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "predictions = pd.read_csv(input_test)\n",
        "predictions.columns = ['predicted_value', 'true_value', 'event_type','num_args', 'label']\n",
        "event_types = predictions['event_type']\n",
        "unique_event_types = list(set(event_types))\n",
        "unique_event_types.sort()\n",
        "\n",
        "stats_dict_types = {r: {'pred': 0, 'actual': 0, 'tp': 0, 'tn':0, 'fp':0, 'fn':0} for r in unique_event_types}\n",
        "stats_dict_lens = {r: {'pred': 0, 'actual': 0, 'tp': 0, 'tn':0, 'fp':0, 'fn':0} for r in [0,1,2,3,4]}\n",
        "for index, row in predictions.iterrows():\n",
        "    y = row['true_value']\n",
        "    p = row['predicted_value']\n",
        "    typeev =row['event_type']\n",
        "    arglen = row['num_args']\n",
        "    if y==p:\n",
        "        if y==1:\n",
        "            stats_dict_types[typeev]['tp']+=1\n",
        "            stats_dict_types[typeev]['pred']+=1\n",
        "            stats_dict_types[typeev]['actual']+=1\n",
        "            \n",
        "            stats_dict_lens[arglen]['tp']+=1\n",
        "            stats_dict_lens[arglen]['pred']+=1\n",
        "            stats_dict_lens[arglen]['actual']+=1\n",
        "        if y==0:\n",
        "            stats_dict_types[typeev]['tn']+=1\n",
        "            \n",
        "            stats_dict_lens[arglen]['tn']+=1\n",
        "    else:\n",
        "        if y==1:\n",
        "            stats_dict_types[typeev]['fn']+=1\n",
        "            stats_dict_types[typeev]['actual']+=1\n",
        "            \n",
        "            stats_dict_lens[arglen]['fn']+=1\n",
        "            stats_dict_lens[arglen]['actual']+=1\n",
        "        if y==0:\n",
        "            stats_dict_types[typeev]['fp']+=1\n",
        "            stats_dict_types[typeev]['pred']+=1\n",
        "            \n",
        "            stats_dict_lens[arglen]['fp']+=1\n",
        "            stats_dict_lens[arglen]['pred']+=1\n",
        "\n",
        "report_length_test = produce_report_ee(stats_dict_lens)\n",
        "report_types_test = produce_report_ee(stats_dict_types)\n",
        "\n",
        "print(\"Performance on test set\")\n",
        "dataf = pd.DataFrame.from_dict(report_types_test['per_class'], orient='index')\n",
        "print(dataf.round(4))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvXcL7WaehGq"
      },
      "source": [
        "Does the argument length of the original event influence the performance?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLeruVqnO6EQ"
      },
      "source": [
        "input  = \"../saved/ee-roberta-base-val_eval.csv\"\n",
        "\n",
        "print(\"Performance on validation set per number of arguments\")\n",
        "dataf = pd.DataFrame.from_dict(report_length_val['per_class'], orient='index')\n",
        "print(dataf.round(4))\n",
        "print()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8GIIqqSfEpU"
      },
      "source": [
        "print(\"Performance on test set per number of arguments\")\n",
        "dataf = pd.DataFrame.from_dict(report_length_val['per_class'], orient='index')\n",
        "print(dataf.round(4))\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmCsozEdfH1G"
      },
      "source": [
        "We notice that for events that have a larger number of arguments performance drops significantly. Instead we have perfect predictions for the zero argument case. \n",
        "\n",
        "\n",
        "> Is this a property of our selected tuple generation method?\n",
        "> Let's generate the tuples for 'Growth'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_L9yzNffvUk"
      },
      "source": [
        "from itertools import permutations, combinations, product, chain\n",
        "from operator import itemgetter\n",
        "templates = {}\n",
        "unique_templates = {}\n",
        "total_templates = 0\n",
        "total_unique_templates = 0\n",
        "non_existent_arg = 0\n",
        "\n",
        "with open('../data/MLEE_train.json', 'r') as infile:\n",
        "  for line in infile:\n",
        "    data = json.loads(line)\n",
        "    e_type = {e['id']: e['type'] for e in data['entities']}\n",
        "    event_trig_map = {ev['id']: ev['trigger'] for ev in data['events']}\n",
        "\n",
        "    for e in data['events']:\n",
        "      structure = []\n",
        "\n",
        "      if e['event_type'] not in templates:\n",
        "        templates[e['event_type']] = []\n",
        "        unique_templates[e['event_type']] = []\n",
        "\n",
        "      for arg in e['arguments']:\n",
        "        role = arg['role'].replace('1', '').replace('2', '').replace('3', '').replace('4', '')\n",
        "\n",
        "        if arg['argument'].startswith('T'):\n",
        "            structure.append((role, e_type[arg['argument']]))\n",
        "        else:\n",
        "            if arg['argument'] in event_trig_map:\n",
        "                structure.append((role, e_type[event_trig_map[arg['argument']]]))\n",
        "            else:\n",
        "                structure = []  # exclude event\n",
        "                non_existent_arg += 1\n",
        "\n",
        "      if structure:\n",
        "          perm_structure = list(permutations(structure, len(structure)))\n",
        "          templates[e['event_type']] += perm_structure\n",
        "          unique_templates[e['event_type']].append(tuple(structure))\n",
        "\n",
        "\n",
        "    # remove duplicates\n",
        "for type_ in templates:\n",
        "  templates[type_] = list(set(templates[type_]))\n",
        "  # unique_templates[type_] = list(set(unique_templates[type_]))\n",
        "\n",
        "  total_templates += len(templates[type_])\n",
        "\n",
        "  unique_templates[type_].sort(key=itemgetter(0))\n",
        "  unique_templates[type_] = list(set(unique_templates[type_]))\n",
        "  total_unique_templates += len(unique_templates[type_])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkXQrS0KzN7Y"
      },
      "source": [
        "event_type = 'Growth'\n",
        "print('Event_templates for:' + str(event_type) )\n",
        "print(\"\\n\".join(\"'{}'\".format(value) for value in unique_templates[event_type]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bkdoLlezh_s"
      },
      "source": [
        "# Step 6: Pipeline Performance\n",
        "\n",
        "Now that all 3 components have been completed and trained, we can use them in a pipeline scenario in order to do end-to-end Event Extraction!\n",
        "\n",
        "The NER component, will generate predictions for named entities and triggers in each sentence, which we can feed into the relation component. \n",
        "\n",
        "The following script will evaluate the RE component when using **predicted** named entities.\n",
        "\n",
        "What do you notice?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yN7zDJjE2m7"
      },
      "source": [
        "from evaluation import eval_re\n",
        "\n",
        "with open('/content/ai4health-nactem/src/config.yaml', 'r') as f:\n",
        "  config = yaml.load(f, Loader=yamlordereddictloader.Loader)\n",
        "\n",
        "results = eval_re('../data/MLEE_val.json', '../saved/re-roberta-base-ner_val_preds.json', config)\n",
        "print(results)\n",
        "\n",
        "#!python evaluation.py --config config.yaml --task re --gold ../data/MLEE_val.json --pred ../saved/re-roberta-base-ner_val_preds.json "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffFK-fVvYXlG"
      },
      "source": [
        "Performance of the model drops significantly, almost 27\\% !"
      ]
    }
  ]
}